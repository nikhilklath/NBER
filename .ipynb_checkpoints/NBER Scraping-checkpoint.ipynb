{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose - To scrape data on authors, their affiliation and reported addresses at the level of each paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np \n",
    "import re\n",
    "import string\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import math\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXE_PATH = r\"F:\\Winter 2020\\chromedriver_win32\\chromedriver.exe\"\n",
    "browser = webdriver.Chrome(executable_path=EXE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"https://www.nber.org/papers?page=1&perPage=100&sortBy=public_date\"\n",
    "browser.get(base)\n",
    "browser.implicitly_wait(10)\n",
    "\n",
    "text = browser.find_element_by_class_name(\"search__info-results\").text\n",
    "pages = int(text[text.find('F') + 2 : text.find('R') - 1]) + 1\n",
    "pages = math.ceil(pages/100)\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_excel(r'F:\\Winter 2020\\NBER_WP.xlsx')\n",
    "#scraped = list(df['paper'])\n",
    "scraped = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "count = 0\n",
    "\n",
    "errors = []\n",
    "\n",
    "url = \"https://www.nber.org/papers?page={q}&perPage=100&sortBy=public_date\"\n",
    "\n",
    "for i in range(1, pages + 1, 1):\n",
    "    \n",
    "    print(i)\n",
    "    \n",
    "    browser.get(url.format(q = str(i)))\n",
    "    browser.implicitly_wait(30)\n",
    "    \n",
    "    paper_links = []\n",
    "    for element in browser.find_elements_by_xpath('//a[contains(@href,\"/papers/\")]'):\n",
    "        paper_link = element.get_attribute('href')\n",
    "        paper_links.append(paper_link)\n",
    "    \n",
    "    paper_links = set(paper_links)\n",
    "    for paper_link in paper_links:\n",
    "        \n",
    "        try:\n",
    "            html = urlopen(paper_link)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "            paper_name = soup.find('h1', class_ = 'page-header__title').text.strip()\n",
    "        \n",
    "            if paper_name not in scraped:\n",
    "                dic[count] = {'paper': paper_name}\n",
    "                print(paper_name)\n",
    "                \n",
    "                paper_month = soup.find('time').text\n",
    "                dic[count]['paper month'] = paper_month\n",
    "                \n",
    "                temp = soup.find('div', class_ = 'page-header__citation-item').text.strip()\n",
    "                dic[count]['paper number'] = temp[temp.find('Paper') + 6:]\n",
    "                \n",
    "                for each in soup.find_all('div', class_ = 'info-grid__item'):\n",
    "                    text = [t.text for t in each.find_all('a')]\n",
    "                    l = len(text)\n",
    "    \n",
    "                    if 'topics' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper topic ' + str(c + 1)] = text[c]\n",
    "                    elif 'programs' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper program ' + str(c + 1)] = text[c]\n",
    "                    elif 'working groups' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper group ' + str(c + 1)] = text[c]\n",
    "                    elif 'projects' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper project ' + str(c + 1)] = text[c]\n",
    "        \n",
    "                authors = soup.find_all('div', class_ = 'page-header__authors')[0].find_all('a')\n",
    "        \n",
    "                num = 1\n",
    "                for author in authors:\n",
    "            \n",
    "                    author_name = author.text\n",
    "                    dic[count]['author ' + str(num)] = author_name\n",
    "            \n",
    "                    author_link = \"https://www.nber.org\" + author.get('href')\n",
    "                    html = urlopen(author_link)\n",
    "                    soup = BeautifulSoup(html, 'lxml')\n",
    "            \n",
    "                    author_position = soup.find_all('div', class_ = 'person-header__title')[0].text.strip()\n",
    "                    dic[count]['author position ' + str(num)] = author_position\n",
    "            \n",
    "                    author_affiliation = soup.find_all('div', class_ = 'person-header__summary')[0].text.strip()\n",
    "                    dic[count]['author affiliation ' + str(num)] = author_affiliation\n",
    "                    \n",
    "                    author_address = soup.find_all('div', 'contact-card__social')[-1].text.strip().replace('\\n', ' ')\n",
    "                    dic[count]['author address ' + str(num)] = author_address\n",
    "            \n",
    "                    num+=1\n",
    "            \n",
    "                count += 1 \n",
    "        except:\n",
    "            errors.append(paper_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame.from_dict(dic).T\n",
    "df.to_excel(r'F:\\Winter 2020\\NBER_WP.xlsx',engine='xlsxwriter')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bitc93c2ab14bb54a9e898317ef2f21e286"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
