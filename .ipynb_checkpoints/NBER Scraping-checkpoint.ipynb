{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose - To scrape data on authors, their affiliation and reported addresses at the level of each paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np \n",
    "import re\n",
    "import string\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import math\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXE_PATH = r\"C:\\Users\\nik596\\Downloads\\chromedriver_win32\\chromedriver.exe\"\n",
    "browser = webdriver.Chrome(executable_path=EXE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"https://www.nber.org/papers?page=1&perPage=100&sortBy=public_date\"\n",
    "browser.get(base)\n",
    "browser.implicitly_wait(10)\n",
    "\n",
    "text = browser.find_element_by_class_name(\"search__info-results\").text\n",
    "pages = int(text[text.find('F') + 2 : text.find('R') - 1]) + 1\n",
    "pages = math.ceil(pages/100)\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_links = []\n",
    "url = \"https://www.nber.org/papers?page={q}&perPage=100&sortBy=public_date\"\n",
    "\n",
    "for i in range(1, pages + 1, 1):\n",
    "    \n",
    "    if i%10 ==0:\n",
    "        print(i)\n",
    "    \n",
    "    browser.get(url.format(q = str(i)))\n",
    "    browser.implicitly_wait(30)\n",
    "    \n",
    "    \n",
    "    for element in browser.find_elements_by_xpath('//a[contains(@href,\"/papers/\")]'):\n",
    "        paper_link = element.get_attribute('href')\n",
    "        paper_links.append(paper_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_links = set(paper_links)\n",
    "with open(r'C:\\Users\\nik596\\Documents\\GitHub\\NBER\\papers.txt','wb') as f:\n",
    "   pickle.dump(paper_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\nik596\\Documents\\GitHub\\NBER\\papers.txt','rb') as f:\n",
    "   paper_links = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 27810\n",
      "Total 30487\n"
     ]
    }
   ],
   "source": [
    "scrape_all =0\n",
    "if scrape_all:\n",
    "    papers = []\n",
    "    scraped= []\n",
    "    dic = {}\n",
    "else:\n",
    "    with open(r'C:\\Users\\nik596\\Documents\\GitHub\\NBER\\NBER_WP1.txt','rb') as f:\n",
    "       papers = pickle.load(f)\n",
    "    df=pd.DataFrame.from_dict(papers).T\n",
    "    scraped = list(df['paper_link'])\n",
    "    print(\"Scraped\", len(scraped))\n",
    "    dic = papers\n",
    "print(\"Total\", len(paper_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left 2677\n",
      "27812\n",
      "Turning Workers into Savers? Incentives, Liquidity, and Choice in 401(k) Plan Design\n",
      "27814\n",
      "Class Struggle Inside the Firm: A Study of German Codetermination\n",
      "['https://www.nber.org/papers/w5290']\n",
      "['https://www.nber.org/papers/w5290', 'https://www.nber.org/papers/w0909']\n"
     ]
    }
   ],
   "source": [
    "count = len(papers)\n",
    "\n",
    "errors = []\n",
    "paper_links = [link for link in paper_links if link not in scraped]\n",
    "print(\"Left\", len(paper_links))\n",
    "for paper_link in paper_links:\n",
    "        \n",
    "        try:\n",
    "            html = urlopen(paper_link)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "            paper_name = soup.find('h1', class_ = 'page-header__title').text.strip()\n",
    "        \n",
    "            if paper_link not in scraped:\n",
    "                #print(paper_link)\n",
    "                dic[count] = {'paper': paper_name}\n",
    "                #print(paper_name)\n",
    "                dic[count]['paper_link'] = paper_link\n",
    "                \n",
    "                paper_month = soup.find('time').text\n",
    "                dic[count]['paper month'] = paper_month\n",
    "                \n",
    "                temp = soup.find('div', class_ = 'page-header__citation-item').text.strip()\n",
    "                dic[count]['paper number'] = temp[temp.find('Paper') + 6:]\n",
    "                \n",
    "                abstract = soup.find('div', class_ = 'page-header__intro-inner').text.strip()\n",
    "                dic[count]['paper abstract'] = abstract\n",
    "                \n",
    "                for each in soup.find_all('div', class_ = 'info-grid__item'):\n",
    "                    text = [t.text for t in each.find_all('a')]\n",
    "                    l = len(text)\n",
    "    \n",
    "                    if 'topics' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper topic ' + str(c + 1)] = text[c]\n",
    "                    elif 'programs' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper program ' + str(c + 1)] = text[c]\n",
    "                    elif 'working groups' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper group ' + str(c + 1)] = text[c]\n",
    "                    elif 'projects' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper project ' + str(c + 1)] = text[c]\n",
    "        \n",
    "                authors = soup.find_all('div', class_ = 'page-header__authors')[0].find_all('a')\n",
    "        \n",
    "                num = 1\n",
    "                for author in authors:\n",
    "            \n",
    "                    author_name = author.text\n",
    "                    author_link = author.get('href')\n",
    "                    dic[count]['author ' + str(num)] = author_name\n",
    "                    dic[count]['author link ' + str(num)] = author_link\n",
    "            \n",
    "                    author_link = \"https://www.nber.org\" + author_link\n",
    "                    html = urlopen(author_link)\n",
    "                    soup = BeautifulSoup(html, 'lxml')\n",
    "                    \n",
    "                    pos = soup.find_all('div', class_ = 'person-header__title')[0]\n",
    "                    if len(pos) != 0:\n",
    "                        author_position = pos.text.strip()\n",
    "                        dic[count]['author position ' + str(num)] = author_position\n",
    "                    else:\n",
    "                        dic[count]['author position ' + str(num)] = \"\"\n",
    "                        \n",
    "                    affil = soup.find_all('div', class_ = 'person-header__summary')\n",
    "                    if len(affil) != 0:\n",
    "                        author_affiliation = affil[0].text.strip()\n",
    "                        dic[count]['author affiliation ' + str(num)] = author_affiliation\n",
    "                    else:\n",
    "                        dic[count]['author affiliation ' + str(num)] = \"\"\n",
    "                        \n",
    "                    add = soup.find_all('div', 'contact-card__social')\n",
    "                    if len(add) != 0:\n",
    "                        author_address = add[-1].text.strip().replace('\\n', ' ')\n",
    "                        dic[count]['author address ' + str(num)] = author_address\n",
    "                    else:\n",
    "                        dic[count]['author address ' + str(num)] = \"\"\n",
    "            \n",
    "                    num+=1\n",
    "                #print(count)\n",
    "                count += 1\n",
    "                if count%50 ==0:\n",
    "                    print(count)\n",
    "                    print(paper_name)\n",
    "                    with open(r'C:\\Users\\nik596\\Documents\\GitHub\\NBER\\NBER_WP1.txt','wb') as f:\n",
    "                       pickle.dump(dic, f)\n",
    "        except:\n",
    "            errors.append(paper_link)\n",
    "            print(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame.from_dict(dic).T\n",
    "df.to_excel(r'C:\\Users\\nik596\\Documents\\GitHub\\NBER\\NBER_WP.xlsx',engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
