{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose - To scrape data on authors, their affiliation and reported addresses at the level of each paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import numpy as np \n",
    "import re\n",
    "import string\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import math\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nik596\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "EXE_PATH = r\"C:\\Users\\nik596\\Downloads\\chromedriver_win32\\chromedriver.exe\"\n",
    "browser = webdriver.Chrome(executable_path=EXE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nik596\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "305"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = \"https://www.nber.org/papers?page=1&perPage=100&sortBy=public_date\"\n",
    "browser.get(base)\n",
    "browser.implicitly_wait(10)\n",
    "\n",
    "text = browser.find_element_by_class_name(\"search__info-results\").text\n",
    "pages = int(text[text.find('F') + 2 : text.find('R') - 1]) + 1\n",
    "pages = math.ceil(pages/100)\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_links = []\n",
    "url = \"https://www.nber.org/papers?page={q}&perPage=100&sortBy=public_date\"\n",
    "\n",
    "for i in range(1, pages + 1, 1):\n",
    "    \n",
    "    if i%10 ==0:\n",
    "        print(i)\n",
    "    \n",
    "    browser.get(url.format(q = str(i)))\n",
    "    browser.implicitly_wait(30)\n",
    "    \n",
    "    \n",
    "    for element in browser.find_elements_by_xpath('//a[contains(@href,\"/papers/\")]'):\n",
    "        paper_link = element.get_attribute('href')\n",
    "        paper_links.append(paper_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_links = set(paper_links)\n",
    "with open(r'C:\\Users\\nik596\\Documents\\GitHub\\NBER\\papers.txt','wb') as f:\n",
    "   pickle.dump(paper_links, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\nik596\\Documents\\GitHub\\NBER\\papers.txt','rb') as f:\n",
    "   paper_links = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3950"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r'C:\\Users\\nik596\\Documents\\GitHub\\NBER\\NBER_WP.txt','rb') as f:\n",
    "   papers = pickle.load(f)\n",
    "df=pd.DataFrame.from_dict(papers).T\n",
    "#df = pd.read_excel(r'C:\\Users\\nik596\\Dropbox (Harvard University)\\New folder\\NBER_WP.xlsx')\n",
    "scraped = list(df['paper_link'])\n",
    "len(scraped)\n",
    "#scraped = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "Pledgeability, Industry Liquidity, and Financing Cycles\n",
      "4050\n",
      "Do Appeals to Donor Benefits Raise More Money than Appeals to Recipient Benefits? Evidence from a Natural Field Experiment with Pick.Click.Give.\n",
      "4100\n",
      "Evaluating the European View that the US has No Unemployment Problem\n",
      "4150\n",
      "Capital Taxation and Ownership when Markets are Incomplete\n",
      "4200\n",
      "Employee Stock Purchase Plans\n",
      "4250\n",
      "How Well do Foreign Exchange Markets Function: Might a Tobin Tax Help?\n",
      "4300\n",
      "Digitization and Pre-Purchase Information: The Causal and Welfare Impacts of Reviews and Crowd Ratings\n",
      "4350\n",
      "India's Lockdown: An Interim Report\n",
      "4400\n",
      "Purchasing-Power Annuities: Financial Innovation for Stable Real Retirement Income in an Inflationary Environment\n",
      "4450\n",
      "Do Enlarged Fiscal Deficits Cause Inflation: The Historical Record\n",
      "4500\n",
      "Commodity Price Volatility in the Biofuel Era: An Examination of the Linkage Between Energy and Agricultural Markets\n",
      "4550\n",
      "Short-run Effects of Parental Job Loss on Child Health\n",
      "4600\n",
      "The Long-run Impact of New Medical Ideas on Cancer Survival and Mortality\n",
      "4650\n",
      "Debt and Corporate Performance: Evidence from Unsuccessful Takeovers\n",
      "4700\n",
      "Residential Segregation in General Equilibrium\n",
      "4750\n",
      "Incorporation, and Productivity\n",
      "4800\n",
      "Which Workers Bear the Burden of Social Distancing?\n",
      "4850\n",
      "Does Retiree Health Insurance Encourage Early Retirement?\n",
      "4900\n",
      "The Political Economy of International Unions\n",
      "4950\n",
      "Stress Testing Networks: The Case of Central Counterparties\n",
      "5000\n",
      "Estimates from a Consumer Demand System: Implications for the Incidence of Environmental Taxes\n",
      "5050\n",
      "Asia-Pacific Capital Markets: Measurement of Integration and the Implications for Economic Activity\n",
      "5100\n",
      "The Carnegie Conjecture:  Some Empirical Evidence\n",
      "5150\n",
      "Labor Demand: What Do We Know? What Don't We Know?\n",
      "5200\n",
      "\"Aggregation Bias\" DOES Explain the PPP Puzzle\n",
      "5250\n",
      "Treasure Hunt: Social Learning in the Field\n",
      "5300\n",
      "Strategic Information Disclosure: The Case of Multi-Attribute Products with Heterogeneous Consumers\n",
      "5350\n",
      "Marijuana legalization and disability claiming\n",
      "5400\n",
      "The Effect of Pollution on Labor Supply: Evidence from a Natural Experiment in Mexico City\n",
      "5450\n",
      "The Economics of Nicotine Consumption\n",
      "5500\n",
      "The Boll Weevilâ€™s Impact on Racial Income Gaps in the Early Twentieth Century\n",
      "5550\n",
      "Party Influence in Congress and the Economy\n",
      "5600\n",
      "Banks and Markets: The Changing Character of European Finance\n",
      "5650\n",
      "An Economic Model of Amniocentesis Choice\n"
     ]
    }
   ],
   "source": [
    "dic = papers\n",
    "count = len(papers)\n",
    "\n",
    "errors = []\n",
    "\n",
    "for paper_link in paper_links:\n",
    "        \n",
    "        try:\n",
    "            html = urlopen(paper_link)\n",
    "            soup = BeautifulSoup(html, 'lxml')\n",
    "        \n",
    "            paper_name = soup.find('h1', class_ = 'page-header__title').text.strip()\n",
    "        \n",
    "            if paper_link not in scraped:\n",
    "                dic[count] = {'paper': paper_name}\n",
    "                #print(paper_name)\n",
    "                dic[count]['paper_link'] = paper_link\n",
    "                \n",
    "                paper_month = soup.find('time').text\n",
    "                dic[count]['paper month'] = paper_month\n",
    "                \n",
    "                temp = soup.find('div', class_ = 'page-header__citation-item').text.strip()\n",
    "                dic[count]['paper number'] = temp[temp.find('Paper') + 6:]\n",
    "                \n",
    "                for each in soup.find_all('div', class_ = 'info-grid__item'):\n",
    "                    text = [t.text for t in each.find_all('a')]\n",
    "                    l = len(text)\n",
    "    \n",
    "                    if 'topics' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper topic ' + str(c + 1)] = text[c]\n",
    "                    elif 'programs' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper program ' + str(c + 1)] = text[c]\n",
    "                    elif 'working groups' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper group ' + str(c + 1)] = text[c]\n",
    "                    elif 'projects' in each.text.lower():\n",
    "                        for c in range(l):\n",
    "                            dic[count]['paper project ' + str(c + 1)] = text[c]\n",
    "        \n",
    "                authors = soup.find_all('div', class_ = 'page-header__authors')[0].find_all('a')\n",
    "        \n",
    "                num = 1\n",
    "                for author in authors:\n",
    "            \n",
    "                    author_name = author.text\n",
    "                    author_link = author.get('href')\n",
    "                    dic[count]['author ' + str(num)] = author_name\n",
    "                    dic[count]['author link ' + str(num)] = author_link\n",
    "            \n",
    "                    author_link = \"https://www.nber.org\" + author_link\n",
    "                    html = urlopen(author_link)\n",
    "                    soup = BeautifulSoup(html, 'lxml')\n",
    "            \n",
    "                    author_position = soup.find_all('div', class_ = 'person-header__title')[0].text.strip()\n",
    "                    dic[count]['author position ' + str(num)] = author_position\n",
    "            \n",
    "                    author_affiliation = soup.find_all('div', class_ = 'person-header__summary')[0].text.strip()\n",
    "                    dic[count]['author affiliation ' + str(num)] = author_affiliation\n",
    "                    \n",
    "                    author_address = soup.find_all('div', 'contact-card__social')[-1].text.strip().replace('\\n', ' ')\n",
    "                    dic[count]['author address ' + str(num)] = author_address\n",
    "            \n",
    "                    num+=1\n",
    "            \n",
    "                count += 1\n",
    "                if count%50 ==0:\n",
    "                    print(count)\n",
    "                    print(paper_name)\n",
    "                    with open(r'C:\\Users\\nik596\\Documents\\GitHub\\NBER\\NBER_WP.txt','wb') as f:\n",
    "                       pickle.dump(dic, f)\n",
    "        except:\n",
    "            errors.append(paper_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame.from_dict(dic).T\n",
    "df.to_excel(r'C:\\Users\\nik596\\Documents\\GitHub\\NBER\\NBER_WP.xlsx',engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
